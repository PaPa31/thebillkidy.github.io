# Pre-req installations:
# https://docs.docker.com/install/linux/docker-ce/ubuntu/
# https://github.com/NVIDIA/nvidia-docker

# Usage:
#        sudo docker build -t horizon_initial_release . 2>&1 | tee stdout
#    or
#        sudo nvidia-docker build -t horizon_initial_release . 2>&1 | tee stdout
#        sudo nvidia-docker run -i -t --rm horizon_initial_release /bin/bash

# Remove all stopped Docker containers:   sudo docker rm $(sudo docker ps -a -q)
# Remove all untagged images:             sudo docker rmi $(sudo docker images -q --filter "dangling=true")

FROM ubuntu:18.04

SHELL ["/bin/bash", "-c"]

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    ca-certificates \
    cmake \
    git \
    sudo \
    software-properties-common \
    vim \
    emacs \
    wget

# Sometimes needed to avoid SSL CA issues.
RUN update-ca-certificates

# Install OpenGL
RUN apt-get install -y python-opengl

ENV HOME /home
WORKDIR ${HOME}/

# Conda Package Manager
RUN wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && \
    chmod +x miniconda.sh && \
    ./miniconda.sh -b -p ${HOME}/miniconda && \
    rm miniconda.sh

# Setting these env var outside of the install script to ensure
# they persist in image
# (See https://stackoverflow.com/questions/33379393/docker-env-vs-run-export)
ENV PATH ${HOME}/miniconda/bin:$PATH
ENV CONDA_PATH ${HOME}/miniconda
ENV LD_LIBRARY_PATH ${CONDA_PATH}/lib:${LD_LIBRARY_PATH}

# Set channels
RUN conda config --add channels conda-forge # For ONNX/tensorboardX
RUN conda config --add channels pytorch # For PyTorch

# Install dependencies
ADD ./requirements.txt requirements.txt
RUN conda install --file requirements.txt
RUN rm requirements.txt

# Build the latest onnx from source
RUN pip install onnx

# Install open ai gym
RUN pip install "gym[classic_control,box2d,atari]"

# Set JAVA_HOME for Spark
ENV JAVA_HOME ${HOME}/miniconda

# Install Spark
RUN wget http://www-eu.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz && \
    tar -xzf spark-2.4.0-bin-hadoop2.7.tgz && \
    mv spark-2.4.0-bin-hadoop2.7 /usr/local/spark

# Environment
ENV SPARK_HOME=/usr/local/spark

# Configure History Server (https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/)
ADD ./spark-defaults.conf /usr/local/spark/conf/spark-defaults.conf
RUN mkdir -p /opt/spark-logs
RUN /usr/local/spark/sbin/start-history-server.sh

# Expose our ports
# 8080 = Spark Master UI
# 8081 = Spark Worker UI
# 7077 = Master Service
EXPOSE 8080
EXPOSE 8081
EXPOSE 7077
EXPOSE 18080
EXPOSE 4040

# Define default command.
CMD ["/bin/bash"]